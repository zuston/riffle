name: Spark SQL Integration Test

on:
  push:
    branches:
      - master
  pull_request:
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}

env:
  CARGO_TERM_COLOR: always
  UNIFFLE_VERSION: "0.10.0"
  SPARK_VERSION: "3.5.0"
  HADOOP_VERSION: "3.2.2"

jobs:
  spark-sql-integration-test:
    runs-on: ubuntu-22.04
    timeout-minutes: 60

    steps:
      - name: Remove unnecessary files
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf "$AGENT_TOOLSDIRECTORY"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install Protoc
        uses: arduino/setup-protoc@v2
        with:
          version: "23.2"

      - name: Install netcat for port checking
        run: sudo apt-get update && sudo apt-get install -y netcat-openbsd

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          java-version: '11'
          distribution: 'temurin'

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'
      
      - name: Download Uniffle Spark Client
        run: |
          mkdir -p /tmp/spark/jars
          cd /tmp
          wget -q https://repo1.maven.org/maven2/org/apache/uniffle/rss-client-spark3-shaded/${UNIFFLE_VERSION}/rss-client-spark3-shaded-${UNIFFLE_VERSION}.jar -O /tmp/spark/jars/rss-client.jar

      - name: Download and setup Uniffle Coordinator
        timeout-minutes: 15
        run: |
          cd /tmp
          UNIFFLE_URL="https://apache.org/dist/uniffle/${UNIFFLE_VERSION}/apache-uniffle-${UNIFFLE_VERSION}-bin.tar.gz"
          echo "Starting download of Uniffle ${UNIFFLE_VERSION}..."
          echo "URL: ${UNIFFLE_URL}"
          
          # Download with timeout, progress, and retry
          if wget --timeout=30 --tries=3 --progress=dot:giga \
            ${UNIFFLE_URL} \
            -O apache-uniffle-${UNIFFLE_VERSION}-bin.tar.gz; then
            echo ""
            echo "Download completed successfully"
          fi
          
          if [ ! -f apache-uniffle-${UNIFFLE_VERSION}-bin.tar.gz ]; then
            echo "ERROR: Download failed, file not found"
            exit 1
          fi
          
          echo "Verifying downloaded file..."
          ls -lh apache-uniffle-${UNIFFLE_VERSION}-bin.tar.gz
          echo "Extracting Uniffle..."
          tar -xzf apache-uniffle-${UNIFFLE_VERSION}-bin.tar.gz
          mv apache-uniffle-${UNIFFLE_VERSION}-hadoop2.8 uniffle
          
          # Create coordinator config
          mkdir -p uniffle/conf
          cat > uniffle/conf/coordinator.conf << 'EOF'
          rss.coordinator.server.port=21000
          rss.coordinator.server.heartbeat.timeout=30000
          rss.coordinator.app.expired=60000
          rss.coordinator.server.heartbeat.interval=10000
          rss.rpc.server.port=19997
          rss.jetty.http.port=19995
          rss.coordinator.server.client-port=21000
          EOF
          echo "Uniffle Coordinator setup completed"

      - name: Start Uniffle Coordinator
        if: always()
        run: |
          if [ -d "/tmp/uniffle" ]; then
            cd /tmp/uniffle
            export JAVA_HOME=$JAVA_HOME
            # Check if coordinator is already running (from Docker)
            if ! nc -z localhost 21000 2>/dev/null; then
              nohup ./bin/start-coordinator.sh > /tmp/uniffle-coordinator/logs/coordinator.log 2>&1 &
              echo $! > /tmp/uniffle-coordinator.pid
              sleep 10
            else
              echo "Coordinator already running (likely from Docker)"
            fi
          fi
        continue-on-error: true

      - name: Wait for Coordinator to be ready
        run: |
          COORDINATOR_READY=false
          for i in {1..30}; do
            if curl -f http://localhost:19995/api/status 2>/dev/null || nc -z localhost 21000 2>/dev/null; then
              echo "Coordinator is ready"
              COORDINATOR_READY=true
              break
            fi
            echo "Waiting for coordinator... ($i/30)"
            sleep 2
          done
          
          if [ "$COORDINATOR_READY" != "true" ]; then
            echo "ERROR: Coordinator failed to start within timeout"
            echo "=== Coordinator logs (last 20 lines) ==="
            if [ -f /tmp/uniffle-coordinator/logs/coordinator.log ]; then
              tail -20 /tmp/uniffle-coordinator/logs/coordinator.log || echo "Log file not found"
            else
              echo "Log file not found at /tmp/uniffle-coordinator/logs/coordinator.log"
            fi
            echo "=== Checking coordinator process ==="
            ps aux | grep -i coordinator | grep -v grep || echo "No coordinator process found"
            echo "=== Checking ports ==="
            netstat -tuln | grep -E ":(19995|19997|21000)" || echo "No coordinator ports listening"
            exit 1
          fi
          
      - name: Setup Rust Cache
        uses: Swatinem/rust-cache@v2
        with:
          workspaces: "."

      - name: Build Riffle Server
        run: |
          cargo build --release --bin riffle-server

      - name: Create directories for riffle servers
        run: |
          mkdir -p /tmp/riffle-server-1/data
          mkdir -p /tmp/riffle-server-2/data
          mkdir -p /tmp/uniffle-coordinator/logs
          mkdir -p /tmp/spark/work

      - name: Create Riffle Server 1 config
        run: |
          cat > /tmp/riffle-server-1/config.toml << 'EOF'
          store_type = "MEMORY_LOCALFILE"
          grpc_port = 21100
          http_port = 19998
          coordinator_quorum = ["localhost:21000"]
          tags = ["riffle2", "datanode", "GRPC", "ss_v5"]

          [memory_store]
          capacity = "2G"
          dashmap_shard_amount = 128

          [localfile_store]
          data_paths = ["/tmp/riffle-server-1/data"]
          healthy_check_min_disks = 0
          disk_max_concurrency = 2000

          [hybrid_store]
          memory_spill_high_watermark = 0.5
          memory_spill_low_watermark = 0.2
          memory_spill_max_concurrency = 1000

          [runtime_config]
          read_thread_num = 20
          write_thread_num = 50
          grpc_thread_num = 50
          http_thread_num = 5
          default_thread_num = 10
          dispatch_thread_num = 5
          EOF

      - name: Create Riffle Server 2 config
        run: |
          cat > /tmp/riffle-server-2/config.toml << 'EOF'
          store_type = "MEMORY_LOCALFILE"
          grpc_port = 21101
          http_port = 19999
          coordinator_quorum = ["localhost:21000"]
          tags = ["riffle2", "datanode", "GRPC", "ss_v5"]

          [memory_store]
          capacity = "2G"
          dashmap_shard_amount = 128

          [localfile_store]
          data_paths = ["/tmp/riffle-server-2/data"]
          healthy_check_min_disks = 0
          disk_max_concurrency = 2000

          [hybrid_store]
          memory_spill_high_watermark = 0.5
          memory_spill_low_watermark = 0.2
          memory_spill_max_concurrency = 1000

          [runtime_config]
          read_thread_num = 20
          write_thread_num = 50
          grpc_thread_num = 50
          http_thread_num = 5
          default_thread_num = 10
          dispatch_thread_num = 5
          EOF

      - name: Start Riffle Server 1
        run: |
          cd /tmp/riffle-server-1
          WORKER_IP=localhost RUST_LOG=info nohup ${{ github.workspace }}/target/release/riffle-server --config config.toml > server1.log 2>&1 &
          echo $! > /tmp/riffle-server-1.pid
          sleep 5

      - name: Start Riffle Server 2
        run: |
          cd /tmp/riffle-server-2
          WORKER_IP=localhost RUST_LOG=info nohup ${{ github.workspace }}/target/release/riffle-server --config config.toml > server2.log 2>&1 &
          echo $! > /tmp/riffle-server-2.pid
          sleep 5

      - name: Verify Riffle Servers are running
        run: |
          sleep 5
          curl -f http://localhost:19998/metrics || echo "Server 1 metrics not ready"
          curl -f http://localhost:19999/metrics || echo "Server 2 metrics not ready"
          ps aux | grep riffle-server | grep -v grep || echo "Servers may not be running"

      - name: Download and setup Spark
        run: |
          cd /tmp
          wget -q https://apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz
          tar -xzf spark-${SPARK_VERSION}-bin-hadoop3.tgz
          mv spark-${SPARK_VERSION}-bin-hadoop3 spark
          
          # Configure Spark to use Riffle
          cat >> spark/conf/spark-defaults.conf << 'EOF'
          spark.shuffle.manager org.apache.spark.shuffle.RssShuffleManager
          spark.rss.coordinator.quorum localhost:21000
          spark.rss.storage.type MEMORY_LOCALFILE
          spark.executor.instances 2
          spark.executor.cores 2
          spark.executor.memory 2g
          spark.sql.shuffle.partitions 4
          EOF
  
      - name: Create Spark SQL test script
        run: |
          cat > /tmp/test_spark_sql.py << 'EOF'
          from pyspark.sql import SparkSession
          from pyspark.sql.functions import col, count, sum as spark_sum
          
          spark = SparkSession.builder \
              .appName("RiffleIntegrationTest") \
              .config("spark.shuffle.manager", "org.apache.spark.shuffle.RssShuffleManager") \
              .config("spark.rss.coordinator.quorum", "localhost:21000") \
              .config("spark.rss.storage.type", "MEMORY_LOCALFILE") \
              .config("spark.executor.instances", "2") \
              .config("spark.executor.cores", "2") \
              .config("spark.executor.memory", "2g") \
              .config("spark.sql.shuffle.partitions", "4") \
              .getOrCreate()
          
          # Create test data
          data = [(1, "Alice", 25), (2, "Bob", 30), (3, "Charlie", 35), (4, "David", 40)]
          df = spark.createDataFrame(data, ["id", "name", "age"])
          
          # Test basic SQL operations
          df.createOrReplaceTempView("people")
          
          result1 = spark.sql("SELECT * FROM people WHERE age > 30")
          print("Query 1 - Filter results:")
          result1.show()
          
          result2 = spark.sql("SELECT name, age FROM people ORDER BY age DESC")
          print("Query 2 - Order by results:")
          result2.show()
          
          result3 = spark.sql("SELECT COUNT(*) as total, AVG(age) as avg_age FROM people")
          print("Query 3 - Aggregate results:")
          result3.show()
          
          # Test join operation (triggers shuffle)
          data2 = [(1, "Engineer"), (2, "Manager"), (3, "Engineer"), (4, "Designer")]
          df2 = spark.createDataFrame(data2, ["id", "role"])
          df2.createOrReplaceTempView("roles")
          
          result4 = spark.sql("""
              SELECT p.name, p.age, r.role 
              FROM people p 
              JOIN roles r ON p.id = r.id
          """)
          print("Query 4 - Join results:")
          result4.show()
          
          spark.stop()
          print("Spark SQL integration test completed successfully!")
          EOF

      - name: Install PySpark
        run: |
          pip install pyspark==${SPARK_VERSION}

      - name: Run Spark SQL Integration Test
        run: |
          cd /tmp/spark
          export SPARK_HOME=/tmp/spark
          export PATH=$SPARK_HOME/bin:$PATH
          
          # Find py4j jar dynamically
          PY4J_JAR=$(find $SPARK_HOME/python/lib -name "py4j-*.zip" | head -1)
          if [ -n "$PY4J_JAR" ]; then
            export PYTHONPATH=$SPARK_HOME/python:$PY4J_JAR:$PYTHONPATH
          else
            export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
          fi
          
          JAR_OPTS="--jars jars/rss-client.jar"
          
          # Try spark-submit first, fallback to direct Python execution
          if ./bin/spark-submit \
            --master local[4] \
            --conf spark.shuffle.manager=org.apache.spark.shuffle.RssShuffleManager \
            --conf spark.rss.coordinator.quorum=localhost:21000 \
            --conf spark.rss.storage.type=MEMORY_LOCALFILE \
            --conf spark.executor.instances=2 \
            --conf spark.executor.cores=2 \
            --conf spark.executor.memory=2g \
            --conf spark.sql.shuffle.partitions=4 \
            $JAR_OPTS \
            /tmp/test_spark_sql.py; then
            echo "Spark SQL test completed successfully via spark-submit"
          else
            echo "spark-submit failed, trying direct Python execution..."
            python3 /tmp/test_spark_sql.py
          fi

      - name: Check Riffle Server logs
        if: always()
        run: |
          echo "=== Riffle Server 1 Logs ==="
          tail -100 /tmp/riffle-server-1/server1.log || echo "No logs found"
          echo "=== Riffle Server 2 Logs ==="
          tail -100 /tmp/riffle-server-2/server2.log || echo "No logs found"

      - name: Cleanup
        if: always()
        run: |
          # Stop riffle servers
          if [ -f /tmp/riffle-server-1.pid ]; then
            kill $(cat /tmp/riffle-server-1.pid) 2>/dev/null || true
          fi
          if [ -f /tmp/riffle-server-2.pid ]; then
            kill $(cat /tmp/riffle-server-2.pid) 2>/dev/null || true
          fi
          pkill -f riffle-server || true
          
          # Stop coordinator
          if [ -f /tmp/uniffle-coordinator.pid ]; then
            kill $(cat /tmp/uniffle-coordinator.pid) 2>/dev/null || true
          fi
          docker stop uniffle-coordinator 2>/dev/null || true
          docker rm uniffle-coordinator 2>/dev/null || true

